{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc504c0f-1736-4dbd-b1cc-75e5a95966df",
   "metadata": {},
   "source": [
    "# Домашнее задание №1 - Изучение методов анализа и классификации данных\n",
    "## Введение\n",
    "В современном цифровом мире сети являются основными артериями, по которым передаются данные. Однако, наряду с легитимным трафиком, по сетям также передается и вредоносная информация. Для обеспечения безопасности и защиты информации необходимо эффективно обнаруживать и блокировать такие угрозы. Источником данных [`datasets/malicious.csv`](./datasets/malicious.csv) для нашего анализа послужил реальный сетевой трафик, содержащий трафик эксфильтрации данных.\n",
    "\n",
    "- Эксфильтрация данных представляет собой скрытую передачу конфиденциальной информации за пределы защищенной сети.\n",
    "\n",
    "## Описание задания\n",
    "Цель данного задания — разработка и обучение моделей машинного обучения для классификации сетевых потоков на вредоносные и безопасные. Для этого мы будем использовать базовый алгоритм классификации, такой как `RandomForest`.\n",
    "\n",
    "В рамках данной работы вам необходимо выполнить ряд действий, давайте приступим!\n",
    "\n",
    "### Подготовка виртуального окружения Python\n",
    "Предварительно нужно установить все зависимые модули. Все необходимые зависимости перечислены в requirements.txt файле. Чтобы установить зависимости, выполните в терминале следующую команду:\n",
    "```sh\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Загрузить данные\n",
    "Предварительно, необходимо загрузить исходные данные из файла [datasets/malicious.csv](datasets/malicious.csv). Можете воспользоваться функцией `load_data`, которая загрузит csv файл в объект pandas.DataFrame.\n",
    "\n",
    "`pandas.DataFrame` — это двумерная табличная структура данных в библиотеке Pandas для языка программирования Python. Она представляет собой таблицу с рядами и столбцами, где каждый столбец может содержать данные различного типа (числа, строки, булевы значения и т.д.), подобно таблицам в реляционных базах данных или электронных таблицах. `pandas.DataFrame`  предоставляет очень удобный функционал по работе с табличными данными, что весьма ценно в области Машинного обучения.\n",
    "\n",
    "### Изучение исходных данных\n",
    "Когда у вас есть любые данные, после их загрузки необходимо провести первичный анализ, чтобы лучше понять их структуру и особенности. Это важно для дальнейшего построения моделей машинного обучения и выполнения различных аналитических задач. Ниже приведены основные шаги для исследования данных:\n",
    "\n",
    "**Шаг 1**: Вывод общей информации о столбцах данных и их типах\n",
    "Для начала, важно понять структуру вашего набора данных: сколько столбцов он содержит, какие у них названия и какие типы данных в них представлены. Это можно сделать с помощью метода `pandas.DataFrame.info()`, который предоставляет подробную информацию о DataFrame, включая количество записей, типы данных каждого столбца и количество ненулевых значений. \n",
    "Этот шаг поможет вам быстро сориентироваться в данных и определить, какие столбцы требуют дальнейшего внимания или предобработки.\n",
    "\n",
    "**Шаг 2**: Вывод общей информации о данных в столбцах\n",
    "Далее, стоит получить сводную статистическую информацию о данных в каждом столбце. Это можно сделать с помощью метода `pandas.DataFrame.describe()`, который предоставляет основные статистические показатели для числовых столбцов, такие как среднее значение, стандартное отклонение, минимальные и максимальные значения, а также квартильные значения.\n",
    "Эти данные дают представление о распределении значений в каждом столбце и помогают выявить аномалии или выбросы.\n",
    "\n",
    "**Шаг 3**: Проверка пропусков данных\n",
    "Пропуски в данных могут значительно повлиять на качество анализа и моделей машинного обучения. Чтобы выявить наличие пропущенных значений, используйте метод `pandas.DataFrame.isnull().sum()`, который покажет количество пропущенных значений в каждом столбце.\n",
    "Этот шаг позволяет определить, какие столбцы требуют обработки пропусков, например, заполнения средними значениями, медианами или удаления строк.\n",
    "\n",
    "**Шаг 4**: Построение распределения исходных данных\n",
    "Для визуализации распределения данных в каждом столбце можно использовать гистограммы: используйте метод `pandas.DataFrame.hist()`. Гистограмма показывает, как распределены значения в столбце, и помогает выявить схему распределения (например, нормальное распределение, смещение и т.д.). Гистограммы дают визуальное представление о плотности данных и позволяют быстро оценить общие тенденции.\n",
    "\n",
    "**Шаг 5**: Построение boxplot\n",
    "Для более детальной визуализации распределения данных можно использовать boxplot (или диаграммы размаха): `pandas.DataFrame.boxplot()`. Boxplot показывает медиану, квартильные значения, а также потенциальные выбросы.\n",
    "`Boxplot` (или диаграмма размаха) — это графический метод отображения статистических данных, который позволяет визуализировать распределение набора данных и выявлять выбросы. Boxplot наглядно показывает центральное значение, разброс и симметрию распределения данных.\n",
    "\n",
    "**Шаг 6**: Построение матрицы корреляции\n",
    "Матрица корреляции помогает понять взаимосвязь между числовыми переменными в вашем наборе данных. Она показывает, как сильно связанные между собой переменные и позволяет выявить возможные зависимости или мультиколлинеарность.\n",
    "Для построения матрицы корреляции используйте функцию `plot_corr_matrix()` для расчета корреляций между переменными, а затем визуализируйте ее с помощью тепловой карты.\n",
    "\n",
    "**Шаг 7**: Определение соотношения классов в исходном наборе данных. Классы записаны в столбце `label`. Для начала определите сколько всего классов есть. После чего подсчитайте итоговое соотношение классов.\n",
    "\n",
    "**Шаг 8**: Подведите итог исследования исходного набора данных, что вы наблюдаете? Какие столбцы из исходного набора данных могут быть использованы для дальнейшего обучения?\n",
    "\n",
    "### Фильтрация данных\n",
    "После выполнения анализа исходного набора данных, можно приступать к обработке исходных данных. Первым шагом чаще всего является фильтрация набора данных, удаление выбросов из исходного набора данных. \n",
    "\n",
    "Фильтрация данных — это ключевой шаг в подготовке данных для анализа и построения моделей машинного обучения. Неправильные или крайние значения могут сильно искажать результаты и приводить к ненадежным выводам. На этом шаге мы рассмотрим два популярных метода фильтрации данных: Boxplot и Z-оценка. Каждый из них имеет свои преимущества и может быть выбран в зависимости от характера данных и целей анализа.\n",
    "\n",
    "Разделить DataFrame относительно классов, чтобы доминирующий класс не повлиял на фильтрацию данных меньших классов.\n",
    "\n",
    "Отфильтруйте DataFrame полученных с прошлых шагов с использованием функций:\n",
    "- `remove_outliers_quartile` фильтрация на основе квартилей, которые мы наблюдали при построение boxplot, значения находящиеся за усами будут отброшены (удаленны из исходного DataFrame)\n",
    "- `remove_outliers_zscore` фильтрация на основе Z-оценки — это мера, которая показывает, насколько значение в наборе данных отклоняется от среднего значения этого набора, выраженная в стандартных отклонениях\n",
    "\n",
    "После чего постройте boxplot полученного нового DataFrame. Что вы видите теперь, что-то изменилось?\n",
    "\n",
    "### Преобразование данных\n",
    "После фильтрации данных, следующим важным шагом является преобразование данных. Преобразование данных необходимо для подготовки данных к анализу и построению моделей машинного обучения. На этом шаге мы рассмотрим два основных метода преобразования данных: `StandardScaler` и `PowerTransformer`. Каждый из этих методов имеет свои особенности и применяется для решения различных задач.\n",
    "\n",
    "Преобразование данных: Зачем это нужно?\n",
    "\n",
    "Преобразование данных помогает нормализовать или стандартизировать данные, устраняя различия в масштабах и распределениях. Это важно для улучшения производительности моделей машинного обучения и повышения точности прогнозов. Преобразование данных может также помочь сделать данные более симметричными и привести их к нормальному распределению. Например, PCA предполагает многомерную нормальность распределения данных, аналогично K-NN чувствителен к масштабу и распределению данных.\n",
    "\n",
    "`StandardScaler` — это метод стандартизации данных, который приводит их к нулевому среднему значению и единичному стандартному отклонению. Этот метод полезен, когда данные имеют разный масштаб или когда признаки имеют разные единицы измерения.\n",
    "\n",
    "`PowerTransformer` — это метод преобразования данных, который применяется для улучшения симметрии распределения данных и приведения их к нормальному распределению. Это особенно полезно, когда данные имеют сильное смещение или отклонения от нормального распределения. В его основе лежит два типа преобразований: Box-Cox и Yeo-Johnson. Box-Cox требует положительных данных, тогда как Yeo-Johnson работает с любыми данными.\n",
    "\n",
    "Воспользуйтесь обоими преобразованиями и постройте распределения признаков, после их преобразования.\n",
    "\n",
    "### Обучение модели RandomForest\n",
    "После фильтрации и преобразования данных мы готовы приступить к обучению модели. Для этого домашнего задания мы выберем модель RandomForest, которая хорошо справляется с задачами классификации и регрессии, обладает высокой устойчивостью к выбросам и способностью выявлять важные признаки.\n",
    "\n",
    "`RandomForest` — это ансамблевый метод машинного обучения, который строит множество деревьев решений и объединяет их прогнозы для улучшения общей производительности. Модель RandomForest является устойчивой к переобучению, так как каждое дерево обучается на случайной подвыборке данных и использует случайный набор признаков.\n",
    "\n",
    "Поскольку у нас имеется только один набор данных, то мы хотим протестировать модель полностью на нем усреднив результаты тестирования, воспользуемся для этого методом кросс-валидации.\n",
    "\n",
    "Кросс-валидация — это метод оценки качества модели, при котором данные делятся на несколько частей (фолдов), и модель обучается на одной части, а тестируется на другой. Процесс повторяется несколько раз, чтобы каждая часть данных использовалась как для обучения, так и для тестирования. В результате мы получаем усреднённую оценку производительности модели, что делает её более надежной. Можете воспользоваться заготовленной функцией `cross_validation`.\n",
    "\n",
    "## Ожидаемый результат\n",
    "В качестве артефакта домашнего задания №1 должен выступать заполненный notebook, в рамках которого были произведены все расписанные шаги с пояснением, а также дамп обученной модели. Получить дамп модели довольно легко, воспользуйтесь `pickle.dumps()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995bf5d-c213-4825-820e-5f0101ceead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cd9cb-36fb-4fd3-a4c7-ef0905bcb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e924e-b8a2-4aa7-8920-a9043f1248d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(df):\n",
    "    # Построение матрицы корреляции\n",
    "    correlation_matrix = df[df.select_dtypes(include=['float64', 'int64']).columns].corr()\n",
    "    \n",
    "    # Визуализация матрицы корреляции\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Матрица корреляции')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6b317-7404-41a5-966f-c299ae8bf6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_quartile(df):\n",
    "    for column in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "def remove_outliers_zscore(df, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=['float64', 'int64'])))\n",
    "    filtered_entries = (z_scores < threshold).all(axis=1)\n",
    "    return df[filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f0004-2673-4824-8eff-56b829fa3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(clf, X, y):\n",
    "    \"\"\"\n",
    "    Выполняет кросс-валидацию для модели RandomForest и возвращает результаты точности на каждом этапе,\n",
    "    а также наилучшую обученную модель с максимальной точностью.\n",
    "\n",
    "    Параметры:\n",
    "    - clf (Модель классификации, например, RandomForest)\n",
    "    - X (DataFrame или массив): Признаки (features) набора данных.\n",
    "    - y (Series или массив): Метки классов (targets) набора данных.\n",
    "\n",
    "    Возвращает:\n",
    "    - accuracy_scores(list): Массив точностей на каждом фолде.\n",
    "    - best_сlf  (тип исходной модели): Наилучший экземпляр полученный на кросс-валидации\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=3)\n",
    "    accuracy_scores = []\n",
    "    clfs = []\n",
    "    for train, test in skf.split(X, y):\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        clfs.append(clf)\n",
    "    return accuracy_scores, clfs[max(enumerate(accuracy_scores),key=lambda x: x[1])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c1a24-8e36-4538-901e-75e730e25660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('datasets/malicious.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
